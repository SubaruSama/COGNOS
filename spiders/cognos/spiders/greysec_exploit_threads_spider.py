import scrapy
from scrapy.spiders import CrawlSpider
from cognos.items import CognosItem
from scrapy.loader import ItemLoader
class GreySecExploitDevSpider(CrawlSpider):
    name = 'grey_sec_exploit_dev_section'
    allowed_domains = ['greysec.net']
    start_urls = [
        'http://greysec.net/forumdisplay.php?fid=46'
    ]
    custom_settings = {
        'LOG_LEVEL': 'INFO'
    }
    

    # 1
    def start_requests(self):
        # TODO: seguir para a próxima página (procurar following links na documentação)
        yield scrapy.Request(url=self.start_urls[0], callback=self.extract_URL_from_forumdisplay)

    # 2
    def extract_URL_from_forumdisplay(self, response):
        css_link_posts_pattern = 'tr.inline_row:nth-child(n) > td:nth-child(n) > div:nth-child(n) > span:nth-child(n) > span a::attr(href)'
        paths = response.selector.css(css_link_posts_pattern).getall()

        for path in paths:
            yield scrapy.Request(
                    url='https://{allowed_domains}/{path}'.format(allowed_domains=self.allowed_domains[0], 
                    path=path
                ), 
                    callback=self.parse
            )

        css_next_page = 'html body div#content div#container div.wrapper div.float_left div.pagination a.pagination_next::attr(href)'
        css_next_page_link = response.css(css_next_page).get()

        if css_next_page_link is not None:
            yield response.follow(css_next_page_link, callback=self.extract_URL_from_forumdisplay)

    def parse(self, response):
        # Logging stuff
        self.logger.info('----------- I\'am at the Page: %s -----------', response.url)
        self.logger.info('----------- Title: %s -----------', response.xpath('/html/head/title/text()').get())

        contents = response.selector.xpath('//*[@id="posts"]/div[*]/div[1]')

        for content in contents:
            loader = ItemLoader(item=CognosItem(), selector=content)

            loader.add_xpath('title', '/html/head/title')
            loader.add_xpath('username', '//*[@id="posts"]/div[*]/div[1]/div[2]/strong/span/a')

            # if content.xpath('./div[2]/strong/span/a/text()').getall() is not None: # Normal User
            #     print(content.xpath('./div[2]/strong/span/a/text()').getall())
            #     loader.add_xpath('username', '//*[@id="posts"]/div[*]/div[1]/div[2]/strong/span/a')
            #     loader.add_value('is_admin', False)
            #     loader.add_value('is_special_user', False)
            # if content.xpath('./div[2]/strong/span/a/span/text()').getall() is not None: # Special User
            #     loader.add_xpath('username', './div[2]/strong/span/a/span')
            #     loader.add_value('is_admin', False)
            #     loader.add_value('is_special_user', True)
            # if content.xpath('./div[2]/strong/span/a/span/strong/text()').getall() is not None: # Admin User
            #     print(content.xpath('./div[2]/strong/span/a/span/strong/text()').getall())
            #     loader.add_xpath('username', '//*[@id="posts"]/div[*]/div[1]/div[2]/strong/span/a/span/strong')
            #     loader.add_value('is_admin', True)
            #     loader.add_value('is_special_user', True)

            loader.add_xpath('posts_quantity', '//*[@id="posts"]/div[*]/div[1]/div[3]/div[1]/i')
            loader.add_xpath('date_joined', '//*[@id="posts"]/div[*]/div[1]/div[3]/div[3]/i')
            loader.add_xpath('reputation', '//*[@id="posts"]/div[*]/div[1]/div[3]/div[4]/i/a/strong')
            loader.add_xpath('info_date_post', '//*[@id="posts"]/div[*]/div[2]/div[1]/span')
            loader.add_xpath('post_content', '//*[@id="posts"]/div[*]/div[2]/div[2]')

        yield loader.load_item()
