import scrapy
from scrapy.spiders import CrawlSpider
from ..items.forum_content import ForumContent
class GreySecExploitDevSpider(CrawlSpider):
    name = 'grey_sec_exploit_dev_section'
    allowed_domains = ['greysec.net']
    start_urls = [
        'http://greysec.net/forumdisplay.php?fid=46'
    ]
    custom_settings = {
        'LOG_LEVEL': 'INFO'
    }
    

    # 1
    def start_requests(self):
        # TODO: seguir para a próxima página (procurar following links na documentação)
        yield scrapy.Request(url=self.start_urls[0], callback=self.extract_URL_from_forumdisplay)

    # 2
    def extract_URL_from_forumdisplay(self, response):
        css_link_posts_pattern = 'tr.inline_row:nth-child(n) > td:nth-child(n) > div:nth-child(n) > span:nth-child(n) > span a::attr(href)'
        paths = response.selector.css(css_link_posts_pattern).getall()

        for path in paths:
            yield scrapy.Request(
                    url='https://{allowed_domains}/{path}'.format(allowed_domains=self.allowed_domains[0], 
                    path=path
                ), 
                    callback=self.parse
            )

        css_next_page = 'html body div#content div#container div.wrapper div.float_left div.pagination a.pagination_next::attr(href)'
        css_next_page_link = response.css(css_next_page).get()

        if css_next_page_link is not None:
            yield response.follow(css_next_page_link, callback=self.extract_URL_from_forumdisplay)

    def parse(self, response):
        # Logging stuff
        self.logger.info('----------- I\'am at the Page: %s -----------', response.url)
        self.logger.info('----------- Title: %s -----------', response.xpath('/html/head/title/text()').get())
        self.logger.info(' -----------Page: %s -----------', 
            response.xpath('//*[@id="container"]/div[4]/div[1]/div/span[2]/text()').get()
        )

        contents = response.selector.xpath('//*[@id="posts"]/div[*]/div[1]')
        item = ForumContent()

        for content in contents:
            item['title'] = content.xpath('/html/head/title/text()').get()
            if content.xpath('./div[2]/strong/span/a/text()').get() is not None: # Indica que a classe não está vazia e é um usuário normal
                item['username'] = '{username}'.format(username=content.xpath('./div[2]/strong/span/a/text()').get())
                item['is_admin'] = False
                item['is_special_user'] = False
            elif content.xpath('./div[2]/strong/span/a/span/text()').get() is not None: # Indica que é um usuário com título especial
                item['username'] = '{username}'.format(username=content.xpath('./div[2]/strong/span/a/span/text()').get())
                item['is_admin'] = False
                item['is_special_user'] = True
            elif content.xpath('./div[2]/strong/span/a/span/strong/text()').get() is not None: # Indica que é um usuário admin
                item['username'] = '{username}'.format(username=content.xpath('./div[2]/strong/span/a/span/strong/text()').get())
                item['is_admin'] = True
                item['is_special_user'] = True
            print(item['username'])
            
            item['posts_quantity'] = '{posts_qtd}'.format(posts_qtd=content.xpath('./div[3]/div/i/text()').get())
            print(item['posts_quantity'])

            item['date_joined'] = '{date_joined}'.format(date_joined=content.xpath('./div[3]/div[3]/i/text()').get())
            print(item['date_joined'])

            item['reputation'] = '{rep}'.format(rep=content.xpath('./div[3]/div[4]/i/a/strong/text()').get())
            print(item['reputation'])

            item['info_date_post'] = '{info_date_post}'.format(info_date_post=contents.xpath('//*[@id="posts"]/div[*]/div[2]/div[1]/span/text()').get())
            print(item['info_date_post'])

            item['post_content'] = '{content}'.format(content=contents.xpath('//*[@id="posts"]/div[*]/div[2]/div[2]/text()').getall())

        yield item
