import scrapy
from scrapy.spiders import CrawlSpider
from ..items.forum_content import ForumContent
class GreySecExploitDevSpider(CrawlSpider):
    name = 'grey_sec_exploit_dev_section'
    allowed_domains = ['greysec.net']
    start_urls = [
        'http://greysec.net/forumdisplay.php?fid=46'
    ]
    custom_settings = {
        'LOG_LEVEL': 'INFO'
    }
    

    def start_requests(self):
        # TODO: seguir para a próxima página (procurar following links na documentação)
        yield scrapy.Request(url=self.start_urls[0], callback=self.extract_URL_from_forumdisplay)

    def extract_URL_from_forumdisplay(self, response):
        css_link_pattern = 'tr.inline_row:nth-child(n) > td:nth-child(n) > div:nth-child(n) > span:nth-child(n) > span a::attr(href)'
        paths = response.selector.css(css_link_pattern).getall()
        for path in paths:
            yield scrapy.Request(
                    url='https://{allowed_domains}/{path}'.format(allowed_domains=self.allowed_domains[0], 
                    path=path
                ), 
                    callback=self.parse
            )

    def parse(self, response):
        self.logger.info('----------- I\'am at the Page: %s -----------', response.url)
        self.logger.info('----------- Title: %s -----------', response.xpath('/html/head/title/text()').get())
        contents = response.selector.xpath('//*[@id="posts"]/div[*]/div[1]')
        item = ForumContent()

        for content in contents:
            if content.xpath('./div[2]/strong/span/a/text()').get() is not None: # Indica que a classe não está vazia e é um usuário normal
                item['username'] = 'Username: {username}'.format(username=content.xpath('./div[2]/strong/span/a/text()').get())
                item['is_admin'] = False
                item['is_special_user'] = False
            elif content.xpath('./div[2]/strong/span/a/span/text()').get() is not None: # Indica que é um usuário com título especial
                item['username'] = 'Username: {username}'.format(username=content.xpath('./div[2]/strong/span/a/span/text()').get())
                item['is_admin'] = False
                item['is_special_user'] = True
            elif content.xpath('./div[2]/strong/span/a/span/strong/text()').get() is not None: # Indica que é um usuário admin
                item['username'] = 'Username: {username}'.format(username=content.xpath('./div[2]/strong/span/a/span/strong/text()').get())
                item['is_admin'] = True
                item['is_special_user'] = True

            item['posts_quantity'] = 'Posts quantity: {posts_qtd}'.format(posts_qtd=content.xpath('./div[3]/div/i/text()').get())
            print(item['posts_quantity'])

            item['date_joined'] = 'Date joined: {date_joined}'.format(date_joined=content.xpath('./div[3]/div[3]/i/text()').get())
            print(item['date_joined'])

            item['reputation'] = 'Reputation: {rep}'.format(rep=content.xpath('./div[3]/div[4]/i/a/strong/text()').get())
            print(item['reputation'])

            item['info_date_post'] = 'Info date post: {info_date_post}'.format(info_date_post=contents.xpath('//*[@id="posts"]/div[*]/div[2]/div[1]/span/text()').get())
            print(item['info_date_post'])

            item['post_content'] = 'Post content: {content}'.format(content=contents.xpath('//*[@id="posts"]/div[*]/div[2]/div[2]/text()').getall())
            #print(item['post_content'])

        yield item
